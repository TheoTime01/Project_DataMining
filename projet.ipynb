{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet DataMining\n",
    "\n",
    "## Description du projet\n",
    "L'objectif de ce projet est de recommander des images de pokémons en fonction des préférences de l'utilisateur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation des bases de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentification\n",
    "_Pour utiliser l' API publique de Kaggle , vous devez d'abord vous authentifier à l'aide d'un jeton d'API. Depuis l'en-tête du site, cliquez sur votre photo de profil d'utilisateur, puis sur \"Mon compte\" dans le menu déroulant. Cela vous amènera aux paramètres de votre compte sur https://www.kaggle.com/account. Faites défiler jusqu'à la section de la page intitulée API :_\n",
    "\n",
    "_Pour créer un nouveau jeton, cliquez sur le bouton \"Créer un nouveau jeton API\". Cela téléchargera un nouveau jeton d'authentification sur votre machine._\n",
    "\n",
    "_Si vous utilisez l'outil Kaggle CLI, l'outil recherchera ce jeton dans __~/.kaggle/kaggle.json sous Linux__, OSX et d'autres systèmes d'exploitation basés sur UNIX, et dans __C:\\Users<Windows-username>.kaggle\\kaggle.json sous Windows__. Si le jeton n'est pas là, une erreur sera levée. Par conséquent, une fois que vous avez téléchargé le jeton, vous devez le déplacer de votre dossier Téléchargements vers ce dossier._\n",
    "\n",
    "_Si vous utilisez directement l'API Kaggle, l'endroit où vous conservez le jeton n'a pas d'importance, tant que vous êtes en mesure de fournir vos informations d'identification au moment de l'exécution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Télécharger le fichier de données\n",
    "kaggle.api.authenticate()\n",
    "kaggle.api.dataset_download_files('kvpratama/pokemon-images-dataset', path='.', unzip=True, quiet=False, force=False)\n",
    "kaggle.api.dataset_download_files('abcsds/pokemon', path='./data_csv', unzip=True, quiet=False, force=False)\n",
    "kaggle.api.dataset_download_files('avi1023/color-names', path='./data_csv', unzip=True, quiet=False, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer le dossier pokemon dans pokemon_img\n",
    "shutil.rmtree('./pokemon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Création de la base de données de pokémons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = r\"./pokemon_jpg/pokemon_jpg\" # chemin vers le répertoire contenant les images\n",
    "\n",
    "# initialise un dictionnaire pour stocker les métadonnées de toutes les images\n",
    "all_metadata = {}\n",
    "color_data={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boucle à travers tous les fichiers d'image dans le répertoire\n",
    "for img_filename in os.listdir(img_dir):\n",
    "    if img_filename.endswith(\".jpg\") or img_filename.endswith(\".png\"):\n",
    "        # construire le chemin complet vers le fichier d'image\n",
    "        img_path = os.path.join(img_dir, img_filename)\n",
    "\n",
    "        # ouvrir le fichier d'image\n",
    "        with Image.open(img_path) as img:\n",
    "\n",
    "            # supprimer les images avec un nom qui ne sont pas de ce format: 1.jpg, 2.jpg, 3.jpg, etc.\n",
    "            if not img_filename.split(\".\")[0].isdigit():\n",
    "                continue\n",
    "\n",
    "            # extraire les métadonnées de l'image\n",
    "            img_filename= img.filename\n",
    "            img_format = img.format\n",
    "            img_size = img.size\n",
    "            img_orientation = \"landscape\" if img_size[0] > img_size[1] else \"portrait\"\n",
    "            creation_date =  datetime.fromtimestamp(os.path.getctime(img_path)).strftime('%d/%m/%Y')\n",
    "\n",
    "            # créer un dictionnaire de métadonnées pour cette image\n",
    "            metadata = {\n",
    "                #on veut juste le nom de l'image\n",
    "                \"id\": int((img_filename.split(\"\\\\\")[-1]).split(\".\")[0]),\n",
    "                \"format\": img_format,\n",
    "                \"size\": img_size,\n",
    "                \"orientation\": img_orientation,\n",
    "                \"creation_date\": creation_date,\n",
    "                \"tags\": \"\"\n",
    "            }\n",
    "\n",
    "            # ajouter les métadonnées de cette image au dictionnaire de toutes les métadonnées\n",
    "            all_metadata[img_filename.split(\"\\\\\")[-1]] = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les métadonnées au fichier JSON\n",
    "with open('database.json', \"w\") as f:\n",
    "    json.dump(all_metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Création de la base de données de couleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger le fichier csv\n",
    "df = pd.read_csv(\"./data_csv/color_names.csv\", sep=',', header=0)\n",
    "\n",
    "# sélectionner les colonnes Name et Hex (24 bit)\n",
    "df_selected = df.loc[:, [\"Name\", \"Hex (24 bit)\"]]\n",
    "df_selected.rename(columns={\"Hex (24 bit)\": \"Hex\"}, inplace=True)\n",
    "\n",
    "# remplacer # par rien\n",
    "df_selected['Hex'] = df_selected['Hex'].str.replace('#', '')\n",
    "\n",
    "with open('color_names.json', 'w') as f:\n",
    "    f.write(df_selected.to_json(orient='records')) \n",
    "\n",
    "with open('color_names.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('color_names.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Ajout des couleurs aux pokémons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boucle à travers tous les fichiers d'image dans le répertoire\n",
    "color_data = {}\n",
    "img_dir = r\"./pokemon_jpg/pokemon_jpg\" \n",
    "for img_filename in os.listdir(img_dir): \n",
    "    if img_filename.endswith(\".jpg\") or img_filename.endswith(\".png\"):\n",
    "        # Construire le chemin complet vers le fichier d'image\n",
    "        img_path = os.path.join(img_dir, img_filename)\n",
    "        # Ouvrir l'image\n",
    "        with Image.open(img_path) as img:\n",
    "            \n",
    "            \n",
    "            # Supprimer les images avec un nom qui ne sont pas de ce format: 1.jpg, 2.jpg, 3.jpg, etc.\n",
    "            if not img_filename.split(\".\")[0].isdigit():\n",
    "                continue\n",
    "\n",
    "            pixel_matrix = np.array(img) # Convertir l'image en matrice de pixels\n",
    "\n",
    "            # Extraire les valeurs R, G, B\n",
    "            pixel_data = pixel_matrix.reshape((-1, 3))\n",
    "\n",
    "            # Utiliser MiniBatchKMeans pour trouver le cluster le plus grand\n",
    "            kmeans = MiniBatchKMeans(n_clusters=4, random_state=0).fit(pixel_data)\n",
    "            main_color = kmeans.cluster_centers_[np.argmax(np.unique(kmeans.labels_, return_counts=True)[1])]\n",
    "            hex_value = \"{0:02X}{1:02X}{2:02X}\".format(int(main_color[0]), int(main_color[1]), int(main_color[2]))\n",
    "\n",
    "\n",
    "            # créer un dictionnaire de couleur pour cette image\n",
    "            color = {\n",
    "                \"id\": int((img_filename.split(\"\\\\\")[-1]).split(\".\")[0]),\n",
    "                \"couleur dominante\": main_color.tolist(),\n",
    "                \"nom couleur\": hex_value\n",
    "            }\n",
    "\n",
    "            # ajouter les couleurs de cette image au dictionnaire de toutes les couleurs\n",
    "            color_data[(img_filename.split(\"\\\\\")[-1]).split(\".\")[0]] = color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les couleurs au fichier JSON\n",
    "with open('color_data.json', 'w') as f:\n",
    "    json.dump(color_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare les couleurs de l'image avec la liste des couleurs du fichier json\n",
    "\n",
    "with open('color_names.json', 'r') as f:\n",
    "    color_names = json.load(f)\n",
    "\n",
    "with open('color_data.json', 'r') as f:\n",
    "    color_data = json.load(f)\n",
    "\n",
    "for pokemon in color_data:\n",
    "    color_hex = color_data[pokemon]['nom couleur']\n",
    "    for color in color_names:\n",
    "        color_rgb = (int(color_hex[1:2], 16), int(color_hex[2:4], 16), int(color_hex[4:], 16))\n",
    "        color_rgb_names = (int(color['Hex'][1:2], 16), int(color['Hex'][2:4], 16), int(color['Hex'][4:], 16))\n",
    "        # verifie si les trois valeurs sont les memes\n",
    "        if (color_rgb[0] == color_rgb_names[0] and color_rgb[1] == color_rgb_names[1] and color_rgb[2] == color_rgb_names[2]):\n",
    "            color_data[pokemon]['nom couleur'] = color['Name']\n",
    "        # verifie si deux des trois valeurs sont les memes\n",
    "        elif ((color_rgb[0] == color_rgb_names[0] and color_rgb[1] == color_rgb_names[1])\n",
    "        or (color_rgb[0] == color_rgb_names[0] and color_rgb[2] == color_rgb_names[2]) \n",
    "        or (color_rgb[1] == color_rgb_names[1] and color_rgb[2] == color_rgb_names[2])):\n",
    "            color_data[pokemon]['nom couleur'] = color['Name']\n",
    "        # verifie si une des trois valeurs est la meme\n",
    "        elif (color_rgb[0] == color_rgb_names[0] or color_rgb[1] == color_rgb_names[1] or color_rgb[2] == color_rgb_names[2]):\n",
    "            color_data[pokemon]['nom couleur'] = color['Name']\n",
    "\n",
    "with open('color_data.json', 'w') as f:\n",
    "    json.dump(color_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajoute les couleurs au fichier json principal\n",
    "with open('database.json', 'r') as f:\n",
    "    pokemon_data = json.load(f)\n",
    "\n",
    "with open('color_data.json', 'r') as f:\n",
    "    color_data = json.load(f)\n",
    "\n",
    "# si l'id du pokemon est le meme que l'id de la couleur, ajoute la couleur au pokemon\n",
    "for pokemon in pokemon_data:\n",
    "    for color in color_data:\n",
    "        if pokemon_data[pokemon]['id'] == color_data[color]['id']:\n",
    "            pokemon_data[pokemon]['couleur dominante'] = color_data[color]['nom couleur']\n",
    "\n",
    "with open('database.json', 'w') as f:\n",
    "    json.dump(pokemon_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données de tags.json dans data_d[\"tags\"]\n",
    "df = pd.read_csv(r\"./data_csv/Pokemon.csv\", sep=',', header=0)\n",
    "df_selected = df.loc[:, ['#','Name', 'Type 1', 'Type 2', 'Generation', 'Legendary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer la colonne # en id\n",
    "df_selected.rename(columns={'#': 'id'}, inplace=True)\n",
    "\n",
    "with open('tags.json', 'w') as f:\n",
    "    f.write(df_selected.to_json(orient='records')) \n",
    "\n",
    "with open('tags.json', 'r') as f:\n",
    "    data_t = json.load(f)\n",
    "\n",
    "with open('tags.json', 'w') as f:\n",
    "    json.dump(data_t, f, indent=4)\n",
    "\n",
    "with open('database.json', 'r') as f:\n",
    "    data_d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données de tags.json dans data_d[\"tags\"]\n",
    "for key in data_d:\n",
    "    for i in range(len(data_t)):\n",
    "        if data_d[key][\"id\"] == data_t[i][\"id\"]:\n",
    "            data_d[key][\"tags\"] = data_t[i]\n",
    "\n",
    "#Enlever l'id de data_d[\"tags\"]\n",
    "for key in data_d:\n",
    "    data_d[key][\"tags\"].pop(\"id\")\n",
    "\n",
    "# Enregistrer les données dans database.json\n",
    "with open('database.json', 'w') as f:\n",
    "    json.dump(data_d, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation des utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randint, choice\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de filtrage en fonction des préférences utilisateur\n",
    "def filter_images(images, Type1, Type2, legendary):\n",
    "    filtered_images = []\n",
    "    for image in images.values():\n",
    "        color = image[\"couleur dominante\"]\n",
    "        type1 = image[\"tags\"][\"Type 1\"]\n",
    "        type2 = image[\"tags\"][\"Type 2\"]\n",
    "        legendaire = image[\"tags\"][\"Legendary\"]\n",
    "        tags= image[\"tags\"]\n",
    "        if (type1 == Type1 or type2 == Type1) and (type1 == Type2 or type2 == Type2) and not(legendaire^legendary):\n",
    "            tags[\"color\"]=color\n",
    "            tags[\"first_letter\"]=tags[\"Name\"][0]\n",
    "            filtered_images.append(tags)\n",
    "    return filtered_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récuperation des nom couleurs dans database.json\n",
    "data_t={}\n",
    "with open(\"database.json\", \"r\") as f:\n",
    "    data=json.load(f)\n",
    "    for i in data:\n",
    "        data_t[data[i][\"id\"]]=data[i]\n",
    "\n",
    "# récuperation des Types dans database.json\n",
    "Type_t=[]\n",
    "for i in data_t:\n",
    "    if data_t[i][\"tags\"][\"Type 1\"] not in Type_t:\n",
    "        Type_t.append(data_t[i][\"tags\"][\"Type 1\"])\n",
    "    if data_t[i][\"tags\"][\"Type 2\"] not in Type_t:\n",
    "        Type_t.append(data_t[i][\"tags\"][\"Type 2\"])\n",
    "\n",
    "# récuperation des couleurs dans database.json\n",
    "color_t=[]\n",
    "for i in data_t:\n",
    "    if data_t[i][\"couleur dominante\"] not in color_t:\n",
    "        color_t.append(data_t[i][\"couleur dominante\"])\n",
    "\n",
    "# récuperation des nom des pokemons dans database.json\n",
    "name_t=[]\n",
    "for i in data_t:\n",
    "    if data_t[i][\"tags\"][\"Name\"] not in name_t:\n",
    "        name_t.append(data_t[i][\"tags\"][\"Name\"])\n",
    "\n",
    "legendary_t=[True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enregistrement des données dans recommendation.json\n",
    "with open(\"recommendation.json\", \"w\") as f:\n",
    "    json.dump({\"Type\":Type_t, \"Color\":color_t, \"Name\":name_t}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation de l'utilisateur\n",
    "\n",
    "favorite_t=[\"NoFavorite\", \"Favorite\"]\n",
    "all_user={}\n",
    "nb_user=100\n",
    "i=0\n",
    "\n",
    "while i<nb_user:\n",
    "    result=[]\n",
    "    Type1=choice(Type_t) # choix du premier type\n",
    "    Type2=choice(Type_t) # choix du deuxième type\n",
    "    legendary=choice(legendary_t)\n",
    "    data=filter_images(data_t, Type1, Type2, legendary) # filtrage des images\n",
    "    if len(data)>0:\n",
    "        for k in range(len(data)): # choix de favorite ou non favorite\n",
    "            result.append(favorite_t[randint(0, len(favorite_t)-1)])\n",
    "        all_user[i]={\"data\":data, \"result\":result}\n",
    "        i+=1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pok=[]\n",
    "like=[]\n",
    "\n",
    "for i in all_user: # création d'un dictionnaire avec les données et les résultats\n",
    "    for k in range(len(all_user[i][\"data\"])):\n",
    "        pok.append(all_user[i][\"data\"][k])\n",
    "        like.append(all_user[i][\"result\"][k])\n",
    "\n",
    "user={\"data\":pok, \"result\":like}\n",
    "\n",
    "# compter le nombre de favorite et non favorite\n",
    "nb_favorite=0\n",
    "nb_nofavorite=0\n",
    "for i in user[\"result\"]:\n",
    "    if i==\"Favorite\":\n",
    "        nb_favorite+=1\n",
    "    else:\n",
    "        nb_nofavorite+=1\n",
    "\n",
    "print(len(user[\"result\"]))\n",
    "print(\"nb_favorite: \", nb_favorite)\n",
    "print(\"nb_nofavorite: \", nb_nofavorite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des données utilisateur\n",
    "with open(\"user.json\", \"w\") as f:\n",
    "    json.dump(user, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer les données de l'utilisateur\n",
    "with open('user.json', \"r\") as f:\n",
    "        user=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "#creation des labels encoder\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "le3 = LabelEncoder()\n",
    "le4 = LabelEncoder()\n",
    "le5 = LabelEncoder()\n",
    "le6 = LabelEncoder()\n",
    "le7 = LabelEncoder()\n",
    "le8 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A.1 Création du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des dataframes\n",
    "data = user[\"data\"]\n",
    "result = user[\"result\"]\n",
    "dataframe = pd.DataFrame(data, columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "resultframe = pd.DataFrame(result, columns=[\"favorite\"])\n",
    "\n",
    "# encoding des données\n",
    "dataframe[\"Name\"] = le1.fit_transform(dataframe[\"Name\"])\n",
    "dataframe[\"Type 1\"] = le2.fit_transform(dataframe[\"Type 1\"])\n",
    "dataframe[\"Type 2\"] = le3.fit_transform(dataframe[\"Type 2\"])\n",
    "dataframe[\"Generation\"] = le4.fit_transform(dataframe[\"Generation\"])\n",
    "dataframe[\"legendary\"] = le5.fit_transform(dataframe[\"legendary\"])\n",
    "dataframe[\"color\"] = le6.fit_transform(dataframe[\"color\"])\n",
    "dataframe[\"first_letter\"] = le7.fit_transform(dataframe[\"first_letter\"])\n",
    "resultframe[\"favorite\"] = le8.fit_transform(resultframe[\"favorite\"])\n",
    "\n",
    "# creation du model\n",
    "dtc.fit(dataframe, resultframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A.2 Visualisation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du modele \n",
    "dot_data = tree.export_graphviz(\n",
    "    dtc,\n",
    "    out_file=None,\n",
    "    feature_names=dataframe.columns,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    class_names=le8.inverse_transform(resultframe.favorite.unique()),\n",
    "    special_characters=True,\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "pydot_graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "img = Image(pydot_graph.create_png())\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A.3 Evaluation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision du modele\n",
    "print(\"Accuracy:\",metrics.accuracy_score(resultframe, dtc.predict(dataframe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction du modele\n",
    "y_pred = dtc.predict(dataframe)\n",
    "\n",
    "# creating confusion matrix\n",
    "cm = confusion_matrix(resultframe, y_pred, normalize='true')\n",
    "    \n",
    "# displaying confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['not favorite', 'favorite'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sauvegarde du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dtc, open(\"decision_tree.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __B.1 Creation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des données utilisateur\n",
    "with open('user.json', 'r') as f:\n",
    "    user_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "le3 = LabelEncoder()\n",
    "le4 = LabelEncoder()\n",
    "le5 = LabelEncoder()\n",
    "le6 = LabelEncoder()\n",
    "le7 = LabelEncoder()\n",
    "le8 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des dataframes\n",
    "data = user_data[\"data\"]\n",
    "result = user_data[\"result\"]\n",
    "dataframe = pd.DataFrame(data, columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "resultframe = pd.DataFrame(result, columns=[\"favorite\"])\n",
    "\n",
    "# encoding des données\n",
    "dataframe[\"Name\"] = le1.fit_transform(dataframe[\"Name\"])\n",
    "dataframe[\"Type 1\"] = le2.fit_transform(dataframe[\"Type 1\"])\n",
    "dataframe[\"Type 2\"] = le3.fit_transform(dataframe[\"Type 2\"])\n",
    "dataframe[\"Generation\"] = le4.fit_transform(dataframe[\"Generation\"])\n",
    "dataframe[\"legendary\"] = le5.fit_transform(dataframe[\"legendary\"])\n",
    "dataframe[\"color\"] = le6.fit_transform(dataframe[\"color\"])\n",
    "dataframe[\"first_letter\"] = le7.fit_transform(dataframe[\"first_letter\"])\n",
    "resultframe[\"favorite\"] = le8.fit_transform(resultframe[\"favorite\"])\n",
    "\n",
    "# Separe les données en données d'entrainement et de test\n",
    "train_dataset = dataframe.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataframe.drop(train_dataset.index)\n",
    "\n",
    "# Separe les labels en labels d'entrainement et de test\n",
    "train_labels = resultframe.sample(frac=0.8, random_state=0)\n",
    "test_labels = resultframe.drop(train_labels.index)\n",
    "\n",
    "# Definie le modele\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(7,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilation du modele\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# entrainement du modele\n",
    "history = model.fit(train_dataset, train_labels, validation_data=(test_dataset, test_labels), epochs=100) # epochs= nombre d'itération\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __B.2 Visualisation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation du modele\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_viz(model, title=\"Neural Network Model for Favorite Pokemon\", view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __B.3 Evaluation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modele\n",
    "test_loss, test_acc = model.evaluate(train_dataset, train_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des courbes d'entrainement et de validation\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Accuracy and Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sauvegarde du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du modele\n",
    "pickle.dump(model, open(\"neural_network.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __C.1 Creation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import plot_tree\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des données utilisateur\n",
    "with open('user.json', 'r') as f:\n",
    "    user_data = json.load(f)\n",
    "\n",
    "# creation des label encoder\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "le3 = LabelEncoder()\n",
    "le4 = LabelEncoder()\n",
    "le5 = LabelEncoder()\n",
    "le6 = LabelEncoder()\n",
    "le7 = LabelEncoder()\n",
    "le8 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des dataframes\n",
    "data = user_data[\"data\"]\n",
    "result = user_data[\"result\"]\n",
    "dataframe = pd.DataFrame(data, columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "resultframe = pd.DataFrame(result, columns=[\"favorite\"])\n",
    "\n",
    "# encoding des données\n",
    "dataframe[\"Name\"] = le1.fit_transform(dataframe[\"Name\"])\n",
    "dataframe[\"Type 1\"] = le2.fit_transform(dataframe[\"Type 1\"])\n",
    "dataframe[\"Type 2\"] = le3.fit_transform(dataframe[\"Type 2\"])\n",
    "dataframe[\"Generation\"] = le4.fit_transform(dataframe[\"Generation\"])\n",
    "dataframe[\"legendary\"] = le5.fit_transform(dataframe[\"legendary\"])\n",
    "dataframe[\"color\"] = le6.fit_transform(dataframe[\"color\"])\n",
    "dataframe[\"first_letter\"] = le7.fit_transform(dataframe[\"first_letter\"])\n",
    "resultframe[\"favorite\"] = le8.fit_transform(resultframe[\"favorite\"])\n",
    "\n",
    "# Separe les données en données d'entrainement et de test\n",
    "train_data = dataframe.sample(frac=0.8, random_state=0)\n",
    "test_data = dataframe.drop(train_data.index)\n",
    "\n",
    "# Separe les labels en labels d'entrainement et de test\n",
    "train_labels = resultframe.sample(frac=0.8, random_state=0)\n",
    "test_labels = resultframe.drop(train_labels.index)\n",
    "\n",
    "# Definie le modele\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# entrainement du modele\n",
    "model.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __C.2 Visualisation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model.estimators_[0], filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __C.3 Evaluation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modele\n",
    "y_pred = model.predict(train_data)\n",
    "print(\"Accuracy:\", accuracy_score(train_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher la matrice de confusion\n",
    "cm = confusion_matrix(train_labels, y_pred, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['not favorite', 'favorite'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sauvegarde du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder le modèle\n",
    "pickle.dump(model, open(\"random_forest.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syteme de recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_pokemon():\n",
    "    satisfait = False # Variable de controle de la boucle\n",
    "\n",
    "    # import data\n",
    "    with open('database.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open('recommendation.json', 'r') as f:\n",
    "        rec= json.load(f)\n",
    "\n",
    "    # importation des modèles\n",
    "    random_forest = pickle.load(open(\"random_forest.pkl\", \"rb\"))\n",
    "    neural_network = pickle.load(open(\"neural_network.pkl\", \"rb\"))\n",
    "    decision_tree = pickle.load(open(\"decision_tree.pkl\", \"rb\"))\n",
    "\n",
    "    # creation des label encoder\n",
    "    le1 = LabelEncoder()\n",
    "    le2 = LabelEncoder()\n",
    "    le3 = LabelEncoder()\n",
    "    le4 = LabelEncoder()\n",
    "    le5 = LabelEncoder()\n",
    "    le6 = LabelEncoder()\n",
    "    le7 = LabelEncoder()\n",
    "\n",
    "    recommandations = []\n",
    "    \n",
    "\n",
    "    while not satisfait:\n",
    "\n",
    "        #interface utilisateur\n",
    "        print(\"Bonjour, je suis un assistant Pokemon, je vais vous aider à choisir votre prochain Pokemon\")\n",
    "        Nom=input(\"Quel est le nom de votre Pokemon parmis ceux-ci : \" + str(rec[\"Name\"]) + \" ? \")\n",
    "        type1=input(\"Quel est le type 1 de votre Pokemon parmis ceux-ci : \" + str(rec[\"Type\"]) + \" ? \")\n",
    "        type2=input(\"Quel est le type 2 de votre Pokemon parmis ceux-ci : \" + str(rec[\"Type\"]) + \" ? \")\n",
    "        generation=int(input(\"Quel est la generation de votre Pokemon parmis celles ci: 1,2,3,4,5,6 ? \"))\n",
    "        legendary=input(\"Est-ce que votre Pokemon est legendaire (True/False) ? \")\n",
    "        couleur=input(\"Quelle est la couleur de votre Pokemon parmis ceux-ci : \" + str(rec[\"Color\"]) + \" ? \")\n",
    "        first_letter=bool(input(\"Quelle est la premiere lettre de votre Pokemon  (en MAJUSCULE) ?\"))\n",
    "\n",
    "        # Créer un dataframe pour le nouveau Pokémon\n",
    "        pokemon = pd.DataFrame([[Nom, type1, type2, generation, legendary, couleur, first_letter]], columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "\n",
    "        # Encoder les données\n",
    "        pokemon[\"Name\"] = le1.fit_transform(pokemon[\"Name\"])\n",
    "        pokemon[\"Type 1\"] = le2.fit_transform(pokemon[\"Type 1\"])\n",
    "        pokemon[\"Type 2\"] = le3.fit_transform(pokemon[\"Type 2\"])\n",
    "        pokemon[\"Generation\"] = le4.fit_transform(pokemon[\"Generation\"])\n",
    "        pokemon[\"legendary\"] = le5.fit_transform(pokemon[\"legendary\"])\n",
    "        pokemon[\"color\"] = le6.fit_transform(pokemon[\"color\"])\n",
    "        pokemon[\"first_letter\"] = le7.fit_transform(pokemon[\"first_letter\"])\n",
    "\n",
    "        \n",
    "\n",
    "        # Prédire si le Pokémon sera favori ou non\n",
    "        prediction1 = random_forest.predict(pokemon)\n",
    "        prediction2 = neural_network.predict(pokemon)\n",
    "        prediction3 = decision_tree.predict(pokemon)\n",
    "\n",
    "        # Si 2 des 3 modèles prédit que le Pokémon sera favori, alors le Pokémon sera favori\n",
    "        if ((prediction1[0] == 0 and prediction3[0] == 0) \n",
    "        or (prediction1[0] == 1 and prediction2[0] > 0.7) \n",
    "        or (prediction2[0] > 0.7 and prediction3[0] == 0)):\n",
    "            print(\"Le Pokémon sera favori\")\n",
    "            # Ajouter les pokémons recommandés à recommandations \n",
    "            for pokemon_id, pokemon_data in data.items():\n",
    "                if (pokemon_data[\"couleur dominante\"] == couleur \n",
    "                        and pokemon_data[\"tags\"][\"Generation\"] == generation \n",
    "                        and ((pokemon_data[\"tags\"][\"Type 1\"] == type1 and pokemon_data[\"tags\"][\"Type 2\"] == type2) \n",
    "                        or (pokemon_data[\"tags\"][\"Type 1\"] == type2 and pokemon_data[\"tags\"][\"Type 2\"] == type1)) \n",
    "                        and not(pokemon_data[\"tags\"][\"Legendary\"]^legendary)):\n",
    "                    recommandations.append(pokemon_data)\n",
    "\n",
    "            # Afficher les pokémons recommandés\n",
    "            print(\"Voici les pokémons recommandés:\")\n",
    "            for id in recommandations:\n",
    "                print(recommandations[id][\"tags\"][\"Name\"])\n",
    "\n",
    "            satisfait = True\n",
    "        else: # Sinon, recommencer\n",
    "            print(\"Le Pokémon ne sera pas favori\")\n",
    "            satisfait = False\n",
    "\n",
    "    # Afficher les pokémons recommandés\n",
    "    print(\"Voici les pokémons recommandés:\")\n",
    "    for id in recommandations:\n",
    "        print(recommandations[id][\"tags\"][\"Name\"])\n",
    "        img = Image.open(\"./pokemon_jpg/\" + id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_pokemon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test des 3 modèles\n",
    "\n",
    "data={\"Name\":\"Pikachu\",\"Type 1\":\"Electric\",\"Type 2\":\"None\",\"Generation\":1,\"legendary\":False,\"color\":\"Yellow\",\"first_letter\":\"P\"}\n",
    "\n",
    "# Créer un dataframe pour le nouveau Pokémon\n",
    "pokemon = pd.DataFrame([data], columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "\n",
    "# Encoder les données\n",
    "pokemon[\"Name\"] = le1.fit_transform(pokemon[\"Name\"])\n",
    "pokemon[\"Type 1\"] = le2.fit_transform(pokemon[\"Type 1\"])\n",
    "pokemon[\"Type 2\"] = le3.fit_transform(pokemon[\"Type 2\"])\n",
    "pokemon[\"Generation\"] = le4.fit_transform(pokemon[\"Generation\"])\n",
    "pokemon[\"legendary\"] = le5.fit_transform(pokemon[\"legendary\"])\n",
    "pokemon[\"color\"] = le6.fit_transform(pokemon[\"color\"])\n",
    "pokemon[\"first_letter\"] = le7.fit_transform(pokemon[\"first_letter\"])\n",
    "\n",
    "#importation des modèles\n",
    "random_forest = pickle.load(open(\"random_forest.pkl\", \"rb\"))\n",
    "neural_network = pickle.load(open(\"neural_network.pkl\", \"rb\"))\n",
    "decision_tree = pickle.load(open(\"decision_tree.pkl\", \"rb\"))\n",
    "\n",
    "# Prédire si le Pokémon sera favori ou non\n",
    "prediction1 = random_forest.predict(pokemon)\n",
    "prediction2 = neural_network.predict(pokemon)\n",
    "prediction3 = decision_tree.predict(pokemon)\n",
    "\n",
    "# Afficher la prédiction\n",
    "if prediction1[0]==0 :\n",
    "    print(\"d'apres le random forest le Pokémon sera favori\")\n",
    "else:\n",
    "    print(\"d'apres le random forest le Pokémon ne sera pas favori\")\n",
    "  \n",
    "if prediction2[0][0] >= 0.7 :\n",
    "    print(\"d'apres le neural network le Pokémon sera favori\")\n",
    "else:\n",
    "    print(\"d'apres le neural network le Pokémon ne sera pas favori\")\n",
    "\n",
    "if prediction3[0]==0 :\n",
    "    print(\"d'apres le decision tree le Pokémon sera favori\")\n",
    "else:\n",
    "    print(\"d'apres le decision tree le Pokémon ne sera pas favori\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e709a68c8bffd8ca64ede960b472aee5cbbab0a7eefe704a97155fcd513aef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
