{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet DataMining\n",
    "\n",
    "## Description du projet\n",
    "L'objectif de ce projet est de recommander des images de pokémons en fonction des préférences de l'utilisateur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation des bases de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentification\n",
    "_Pour utiliser l' API publique de Kaggle , vous devez d'abord vous authentifier à l'aide d'un jeton d'API. Depuis l'en-tête du site, cliquez sur votre photo de profil d'utilisateur, puis sur \"Mon compte\" dans le menu déroulant. Cela vous amènera aux paramètres de votre compte sur https://www.kaggle.com/account. Faites défiler jusqu'à la section de la page intitulée API :_\n",
    "\n",
    "_Pour créer un nouveau jeton, cliquez sur le bouton \"Créer un nouveau jeton API\". Cela téléchargera un nouveau jeton d'authentification sur votre machine._\n",
    "\n",
    "_Si vous utilisez l'outil Kaggle CLI, l'outil recherchera ce jeton dans __~/.kaggle/kaggle.json sous Linux__, OSX et d'autres systèmes d'exploitation basés sur UNIX, et dans __C:\\Users<Windows-username>.kaggle\\kaggle.json sous Windows__. Si le jeton n'est pas là, une erreur sera levée. Par conséquent, une fois que vous avez téléchargé le jeton, vous devez le déplacer de votre dossier Téléchargements vers ce dossier._\n",
    "\n",
    "_Si vous utilisez directement l'API Kaggle, l'endroit où vous conservez le jeton n'a pas d'importance, tant que vous êtes en mesure de fournir vos informations d'identification au moment de l'exécution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Télécharger le fichier de données\n",
    "kaggle.api.authenticate()\n",
    "kaggle.api.dataset_download_files('kvpratama/pokemon-images-dataset', path='.', unzip=True, quiet=False, force=False)\n",
    "kaggle.api.dataset_download_files('abcsds/pokemon', path='./data_csv', unzip=True, quiet=False, force=False)\n",
    "kaggle.api.dataset_download_files('avi1023/color-names', path='./data_csv', unzip=True, quiet=False, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer le dossier pokemon dans pokemon_img\n",
    "shutil.rmtree('./pokemon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Création de la base de données de pokémons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = r\"./pokemon_jpg/pokemon_jpg\" # chemin vers le répertoire contenant les images\n",
    "\n",
    "# initialise un dictionnaire pour stocker les métadonnées de toutes les images\n",
    "all_metadata = {}\n",
    "color_data={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boucle à travers tous les fichiers d'image dans le répertoire\n",
    "for img_filename in os.listdir(img_dir):\n",
    "    if img_filename.endswith(\".jpg\") or img_filename.endswith(\".png\"):\n",
    "        # construire le chemin complet vers le fichier d'image\n",
    "        img_path = os.path.join(img_dir, img_filename)\n",
    "\n",
    "        # ouvrir le fichier d'image\n",
    "        with Image.open(img_path) as img:\n",
    "\n",
    "            # supprimer les images avec un nom qui ne sont pas de ce format: 1.jpg, 2.jpg, 3.jpg, etc.\n",
    "            if not img_filename.split(\".\")[0].isdigit():\n",
    "                continue\n",
    "\n",
    "            # extraire les métadonnées de l'image\n",
    "            img_filename= img.filename\n",
    "            img_format = img.format\n",
    "            img_size = img.size\n",
    "            img_orientation = \"landscape\" if img_size[0] > img_size[1] else \"portrait\"\n",
    "            creation_date =  datetime.fromtimestamp(os.path.getctime(img_path)).strftime('%d/%m/%Y')\n",
    "\n",
    "            # créer un dictionnaire de métadonnées pour cette image\n",
    "            metadata = {\n",
    "                #on veut juste le nom de l'image\n",
    "                \"id\": int((img_filename.split(\"\\\\\")[-1]).split(\".\")[0]),\n",
    "                \"format\": img_format,\n",
    "                \"size\": img_size,\n",
    "                \"orientation\": img_orientation,\n",
    "                \"creation_date\": creation_date,\n",
    "                \"tags\": \"\"\n",
    "            }\n",
    "\n",
    "            # ajouter les métadonnées de cette image au dictionnaire de toutes les métadonnées\n",
    "            all_metadata[img_filename.split(\"\\\\\")[-1]] = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les métadonnées au fichier JSON\n",
    "with open('database.json', \"w\") as f:\n",
    "    json.dump(all_metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Création de la base de données de couleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger le fichier csv\n",
    "df = pd.read_csv(\"./data_csv/color_names.csv\", sep=',', header=0)\n",
    "\n",
    "# sélectionner les colonnes Name et Hex (24 bit)\n",
    "df_selected = df.loc[:, [\"Name\", \"Hex (24 bit)\"]]\n",
    "df_selected.rename(columns={\"Hex (24 bit)\": \"Hex\"}, inplace=True)\n",
    "\n",
    "# remplacer # par rien\n",
    "df_selected['Hex'] = df_selected['Hex'].str.replace('#', '')\n",
    "\n",
    "with open('color_names.json', 'w') as f:\n",
    "    f.write(df_selected.to_json(orient='records')) \n",
    "\n",
    "with open('color_names.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('color_names.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Ajout des couleurs aux pokémons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boucle à travers tous les fichiers d'image dans le répertoire\n",
    "color_data = {}\n",
    "img_dir = r\"./pokemon_jpg/pokemon_jpg\" \n",
    "for img_filename in os.listdir(img_dir): \n",
    "    if img_filename.endswith(\".jpg\") or img_filename.endswith(\".png\"):\n",
    "        # Construire le chemin complet vers le fichier d'image\n",
    "        img_path = os.path.join(img_dir, img_filename)\n",
    "        # Ouvrir l'image\n",
    "        with Image.open(img_path) as img:\n",
    "            \n",
    "            \n",
    "            # Supprimer les images avec un nom qui ne sont pas de ce format: 1.jpg, 2.jpg, 3.jpg, etc.\n",
    "            if not img_filename.split(\".\")[0].isdigit():\n",
    "                continue\n",
    "\n",
    "            pixel_matrix = np.array(img) # Convertir l'image en matrice de pixels\n",
    "\n",
    "            # Extraire les valeurs R, G, B\n",
    "            pixel_data = pixel_matrix.reshape((-1, 3))\n",
    "\n",
    "            # Utiliser MiniBatchKMeans pour trouver le cluster le plus grand\n",
    "            kmeans = MiniBatchKMeans(n_clusters=4, random_state=0).fit(pixel_data)\n",
    "            main_color = kmeans.cluster_centers_[np.argmax(np.unique(kmeans.labels_, return_counts=True)[1])]\n",
    "            hex_value = \"{0:02X}{1:02X}{2:02X}\".format(int(main_color[0]), int(main_color[1]), int(main_color[2]))\n",
    "\n",
    "\n",
    "            # créer un dictionnaire de couleur pour cette image\n",
    "            color = {\n",
    "                \"id\": int((img_filename.split(\"\\\\\")[-1]).split(\".\")[0]),\n",
    "                \"couleur dominante\": main_color.tolist(),\n",
    "                \"nom couleur\": hex_value\n",
    "            }\n",
    "\n",
    "            # ajouter les couleurs de cette image au dictionnaire de toutes les couleurs\n",
    "            color_data[(img_filename.split(\"\\\\\")[-1]).split(\".\")[0]] = color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les couleurs au fichier JSON\n",
    "with open('color_data.json', 'w') as f:\n",
    "    json.dump(color_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare les couleurs de l'image avec la liste des couleurs du fichier json\n",
    "\n",
    "with open('color_names.json', 'r') as f:\n",
    "    color_names = json.load(f)\n",
    "\n",
    "with open('color_data.json', 'r') as f:\n",
    "    color_data = json.load(f)\n",
    "\n",
    "for pokemon in color_data:\n",
    "    color_hex = color_data[pokemon]['nom couleur']\n",
    "    for color in color_names:\n",
    "        color_rgb = (int(color_hex[1:2], 16), int(color_hex[2:4], 16), int(color_hex[4:], 16))\n",
    "        color_rgb_names = (int(color['Hex'][1:2], 16), int(color['Hex'][2:4], 16), int(color['Hex'][4:], 16))\n",
    "        # verifie si les trois valeurs sont les memes\n",
    "        if (color_rgb[0] == color_rgb_names[0] and color_rgb[1] == color_rgb_names[1] and color_rgb[2] == color_rgb_names[2]):\n",
    "            color_data[pokemon]['nom couleur'] = color['Name']\n",
    "        # verifie si deux des trois valeurs sont les memes\n",
    "        elif ((color_rgb[0] == color_rgb_names[0] and color_rgb[1] == color_rgb_names[1])\n",
    "        or (color_rgb[0] == color_rgb_names[0] and color_rgb[2] == color_rgb_names[2]) \n",
    "        or (color_rgb[1] == color_rgb_names[1] and color_rgb[2] == color_rgb_names[2])):\n",
    "            color_data[pokemon]['nom couleur'] = color['Name']\n",
    "        # verifie si une des trois valeurs est la meme\n",
    "        elif (color_rgb[0] == color_rgb_names[0] or color_rgb[1] == color_rgb_names[1] or color_rgb[2] == color_rgb_names[2]):\n",
    "            color_data[pokemon]['nom couleur'] = color['Name']\n",
    "\n",
    "with open('color_data.json', 'w') as f:\n",
    "    json.dump(color_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajoute les couleurs au fichier json principal\n",
    "with open('database.json', 'r') as f:\n",
    "    pokemon_data = json.load(f)\n",
    "\n",
    "with open('color_data.json', 'r') as f:\n",
    "    color_data = json.load(f)\n",
    "\n",
    "# si l'id du pokemon est le meme que l'id de la couleur, ajoute la couleur au pokemon\n",
    "for pokemon in pokemon_data:\n",
    "    for color in color_data:\n",
    "        if pokemon_data[pokemon]['id'] == color_data[color]['id']:\n",
    "            pokemon_data[pokemon]['couleur dominante'] = color_data[color]['nom couleur']\n",
    "\n",
    "with open('database.json', 'w') as f:\n",
    "    json.dump(pokemon_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données de tags.json dans data_d[\"tags\"]\n",
    "df = pd.read_csv(r\"./data_csv/Pokemon.csv\", sep=',', header=0)\n",
    "df_selected = df.loc[:, ['#','Name', 'Type 1', 'Type 2', 'Generation', 'Legendary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer la colonne # en id\n",
    "df_selected.rename(columns={'#': 'id'}, inplace=True)\n",
    "\n",
    "with open('tags.json', 'w') as f:\n",
    "    f.write(df_selected.to_json(orient='records')) \n",
    "\n",
    "with open('tags.json', 'r') as f:\n",
    "    data_t = json.load(f)\n",
    "\n",
    "with open('tags.json', 'w') as f:\n",
    "    json.dump(data_t, f, indent=4)\n",
    "\n",
    "with open('database.json', 'r') as f:\n",
    "    data_d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données de tags.json dans data_d[\"tags\"]\n",
    "for key in data_d:\n",
    "    for i in range(len(data_t)):\n",
    "        if data_d[key][\"id\"] == data_t[i][\"id\"]:\n",
    "            data_d[key][\"tags\"] = data_t[i]\n",
    "\n",
    "#Enlever l'id de data_d[\"tags\"]\n",
    "for key in data_d:\n",
    "    data_d[key][\"tags\"].pop(\"id\")\n",
    "\n",
    "# Enregistrer les données dans database.json\n",
    "with open('database.json', 'w') as f:\n",
    "    json.dump(data_d, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation des utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randint, choice\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de filtrage en fonction des préférences utilisateur\n",
    "def filter_images(images, Type1, Type2, legendary):\n",
    "    filtered_images = []\n",
    "    for image in images.values():\n",
    "        color = image[\"couleur dominante\"]\n",
    "        type1 = image[\"tags\"][\"Type 1\"]\n",
    "        type2 = image[\"tags\"][\"Type 2\"]\n",
    "        legendaire = image[\"tags\"][\"Legendary\"]\n",
    "        tags= image[\"tags\"]\n",
    "        if (type1 == Type1 or type2 == Type1) and (type1 == Type2 or type2 == Type2) and not(legendaire^legendary):\n",
    "            tags[\"color\"]=color\n",
    "            tags[\"first_letter\"]=tags[\"Name\"][0]\n",
    "            filtered_images.append(tags)\n",
    "    return filtered_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récuperation des nom couleurs dans database.json\n",
    "data_t={}\n",
    "with open(\"database.json\", \"r\") as f:\n",
    "    data=json.load(f)\n",
    "    for i in data:\n",
    "        data_t[data[i][\"id\"]]=data[i]\n",
    "\n",
    "Type_t=[]\n",
    "for i in data_t:\n",
    "    if data_t[i][\"tags\"][\"Type 1\"] not in Type_t:\n",
    "        Type_t.append(data_t[i][\"tags\"][\"Type 1\"])\n",
    "    if data_t[i][\"tags\"][\"Type 2\"] not in Type_t:\n",
    "        Type_t.append(data_t[i][\"tags\"][\"Type 2\"])\n",
    "\n",
    "legendary_t=[True, False]\n",
    "print(Type_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation de l'utilisateur\n",
    "\n",
    "favorite_t=[\"favorite\",\"notfavorite\"]\n",
    "all_user={}\n",
    "nb_user=100\n",
    "i=0\n",
    "\n",
    "while i<nb_user:\n",
    "    result=[]\n",
    "    Type1=choice(Type_t)\n",
    "    Type2=choice(Type_t)\n",
    "    print(Type1)\n",
    "    print(Type2)\n",
    "    legendary=choice(legendary_t)\n",
    "    print(legendary)\n",
    "    data=filter_images(data_t, Type1, Type2, legendary)\n",
    "    if len(data)>0:\n",
    "        for k in range(len(data)):\n",
    "            result.append(favorite_t[randint(0, len(favorite_t)-1)])\n",
    "        all_user[i]={\"data\":data, \"result\":result}\n",
    "        i+=1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pok=[]\n",
    "like=[]\n",
    "\n",
    "for i in all_user:\n",
    "    for k in range(len(all_user[i][\"data\"])):\n",
    "        pok.append(all_user[i][\"data\"][k])\n",
    "        like.append(all_user[i][\"result\"][k])\n",
    "\n",
    "user={\"data\":pok, \"result\":like}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des données utilisateur\n",
    "with open(\"user.json\", \"w\") as f:\n",
    "    json.dump(user, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer les données de l'utilisateur\n",
    "with open('user.json', \"r\") as f:\n",
    "        user=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "\n",
    "#creation des labels encoder\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "le3 = LabelEncoder()\n",
    "le4 = LabelEncoder()\n",
    "le5 = LabelEncoder()\n",
    "le6 = LabelEncoder()\n",
    "le7 = LabelEncoder()\n",
    "le8 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A.1 Création du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes\n",
    "data = user[\"data\"]\n",
    "result = user[\"result\"]\n",
    "dataframe = pd.DataFrame(data, columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "resultframe = pd.DataFrame(result, columns=[\"favorite\"])\n",
    "\n",
    "# generating numerical labels\n",
    "dataframe[\"Name\"] = le1.fit_transform(dataframe[\"Name\"])\n",
    "dataframe[\"Type 1\"] = le2.fit_transform(dataframe[\"Type 1\"])\n",
    "dataframe[\"Type 2\"] = le3.fit_transform(dataframe[\"Type 2\"])\n",
    "dataframe[\"Generation\"] = le4.fit_transform(dataframe[\"Generation\"])\n",
    "dataframe[\"legendary\"] = le5.fit_transform(dataframe[\"legendary\"])\n",
    "dataframe[\"color\"] = le6.fit_transform(dataframe[\"color\"])\n",
    "dataframe[\"first_letter\"] = le7.fit_transform(dataframe[\"first_letter\"])\n",
    "resultframe[\"favorite\"] = le8.fit_transform(resultframe[\"favorite\"])\n",
    "\n",
    "# training the model\n",
    "dtc.fit(dataframe, resultframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A.2 Visualisation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(\n",
    "    dtc,\n",
    "    out_file=None,\n",
    "    feature_names=dataframe.columns,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    class_names=le8.inverse_transform(resultframe.favorite.unique()),\n",
    "    special_characters=True,\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "pydot_graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "img = Image(pydot_graph.create_png())\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A.3 Evaluation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(resultframe, dtc.predict(dataframe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating predictions\n",
    "y_pred = dtc.predict(dataframe)\n",
    "\n",
    "# creating confusion matrix\n",
    "cm = confusion_matrix(resultframe, y_pred, normalize='true')\n",
    "    \n",
    "# displaying confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['not favorite', 'favorite'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sauvegarde du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dtc, open(\"decision_tree.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __B.1 Creation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user input data\n",
    "with open('user.json', 'r') as f:\n",
    "    user_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "le3 = LabelEncoder()\n",
    "le4 = LabelEncoder()\n",
    "le5 = LabelEncoder()\n",
    "le6 = LabelEncoder()\n",
    "le7 = LabelEncoder()\n",
    "le8 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = user_data[\"data\"]\n",
    "result = user_data[\"result\"]\n",
    "dataframe = pd.DataFrame(data, columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "resultframe = pd.DataFrame(result, columns=[\"favorite\"])\n",
    "\n",
    "# Generate numerical labels\n",
    "dataframe[\"Name\"] = le1.fit_transform(dataframe[\"Name\"])\n",
    "dataframe[\"Type 1\"] = le2.fit_transform(dataframe[\"Type 1\"])\n",
    "dataframe[\"Type 2\"] = le3.fit_transform(dataframe[\"Type 2\"])\n",
    "dataframe[\"Generation\"] = le4.fit_transform(dataframe[\"Generation\"])\n",
    "dataframe[\"legendary\"] = le5.fit_transform(dataframe[\"legendary\"])\n",
    "dataframe[\"color\"] = le6.fit_transform(dataframe[\"color\"])\n",
    "dataframe[\"first_letter\"] = le7.fit_transform(dataframe[\"first_letter\"])\n",
    "resultframe[\"favorite\"] = le8.fit_transform(resultframe[\"favorite\"])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_dataset = dataframe.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataframe.drop(train_dataset.index)\n",
    "\n",
    "# Split labels into training and testing sets\n",
    "train_labels = resultframe.sample(frac=0.8, random_state=0)\n",
    "test_labels = resultframe.drop(train_labels.index)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(7,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, train_labels, validation_data=(test_dataset, test_labels), epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __B.2 Visualisation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the neural network\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __B.3 Evaluation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(train_dataset, train_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss over time\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Accuracy and Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sauvegarde du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pickle.dump(model, open(\"neural_network.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __C.1 Creation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import plot_tree\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user input data\n",
    "with open('user.json', 'r') as f:\n",
    "    user_data = json.load(f)\n",
    "\n",
    "# Create label encoders\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "le3 = LabelEncoder()\n",
    "le4 = LabelEncoder()\n",
    "le5 = LabelEncoder()\n",
    "le6 = LabelEncoder()\n",
    "le7 = LabelEncoder()\n",
    "le8 = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes\n",
    "data = user_data[\"data\"]\n",
    "result = user_data[\"result\"]\n",
    "dataframe = pd.DataFrame(data, columns=[\"Name\",\"Type 1\", \"Type 2\", \"Generation\",\"legendary\", \"color\",\"first_letter\"])\n",
    "resultframe = pd.DataFrame(result, columns=[\"favorite\"])\n",
    "\n",
    "# Generate numerical labels\n",
    "dataframe[\"Name\"] = le1.fit_transform(dataframe[\"Name\"])\n",
    "dataframe[\"Type 1\"] = le2.fit_transform(dataframe[\"Type 1\"])\n",
    "dataframe[\"Type 2\"] = le3.fit_transform(dataframe[\"Type 2\"])\n",
    "dataframe[\"Generation\"] = le4.fit_transform(dataframe[\"Generation\"])\n",
    "dataframe[\"legendary\"] = le5.fit_transform(dataframe[\"legendary\"])\n",
    "dataframe[\"color\"] = le6.fit_transform(dataframe[\"color\"])\n",
    "dataframe[\"first_letter\"] = le7.fit_transform(dataframe[\"first_letter\"])\n",
    "resultframe[\"favorite\"] = le8.fit_transform(resultframe[\"favorite\"])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data = dataframe.sample(frac=0.8, random_state=0)\n",
    "test_data = dataframe.drop(train_data.index)\n",
    "\n",
    "# Split labels into training and testing sets\n",
    "train_labels = resultframe.sample(frac=0.8, random_state=0)\n",
    "test_labels = resultframe.drop(train_labels.index)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __C.2 Visualisation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model.estimators_[0], filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __C.3 Evaluation du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = model.predict(train_data)\n",
    "print(\"Accuracy:\", accuracy_score(train_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher la matrice de confusion\n",
    "cm = confusion_matrix(train_labels, y_pred, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['not favorite', 'favorite'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sauvegarde du modèle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder le modèle\n",
    "pickle.dump(model, open(\"random_forest.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syteme de recommandation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_pokemon():\n",
    "    satisfait = False\n",
    "\n",
    "    # Load user input data\n",
    "    with open('data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # import model \n",
    "    random_forest = pickle.load(open(\"random_forest.pkl\", \"rb\"))\n",
    "    neural_network = pickle.load(open(\"neural_network.pkl\", \"rb\"))\n",
    "    decision_tree = pickle.load(open(\"decision_tree.pkl\", \"rb\"))\n",
    "\n",
    "    # Create label encoders\n",
    "    le1 = LabelEncoder()\n",
    "    le2 = LabelEncoder()\n",
    "    le3 = LabelEncoder()\n",
    "    le4 = LabelEncoder()\n",
    "    le5 = LabelEncoder()\n",
    "    le6 = LabelEncoder()\n",
    "\n",
    "    recommandations = []\n",
    "\n",
    "    while not satisfait:\n",
    "        # Interface utilisateur\n",
    "        couleur = input(\"Quelle est votre couleur préférée? \")\n",
    "        generation = int(input(\"Quelle est votre génération préférée? \"))\n",
    "        type1 = input(\"Quel est votre type 1 préféré? \")\n",
    "        type2 = input(\"Quel est votre type 2 préféré? \")\n",
    "        legendary = bool(input(\"Aimez-vous les pokémons légendaires? \"))\n",
    "        first_letter = input(\"Quelle est la première lettre de votre pokémon préféré? \")\n",
    "\n",
    "\n",
    "        # Créer un dataframe pour le nouveau Pokémon\n",
    "        pokemon = pd.DataFrame([[first_letter, type1, type2, generation, legendary, couleur]], columns=[\"first_letter\", \"Type 1\", \"Type 2\", \"Generation\", \"legendary\", \"color\"])\n",
    "\n",
    "        pokemon[\"Type 1\"] = le1.fit_transform(pokemon[\"Type 1\"].values)\n",
    "        pokemon[\"Type 2\"] = le2.fit_transform(pokemon[\"Type 2\"].values)\n",
    "        pokemon[\"color\"] = le3.fit_transform(pokemon[\"color\"].values)\n",
    "        pokemon[\"Generation\"] = le4.fit_transform(pokemon[\"Generation\"].values)\n",
    "        pokemon[\"legendary\"] = le5.fit_transform(pokemon[\"legendary\"].values)\n",
    "        pokemon[\"first_letter\"] = le6.fit_transform(pokemon[\"first_letter\"].values)\n",
    "\n",
    "\n",
    "        # Prédire si le Pokémon sera favori ou non\n",
    "        prediction1 = random_forest.predict(pokemon)\n",
    "        prediction2 = neural_network.predict(pokemon)\n",
    "        prediction3 = decision_tree.predict(pokemon)\n",
    "\n",
    "        # Afficher la prédiction\n",
    "        if prediction1[0] == 0 and prediction2[0] == 0 and prediction3[0] == 0:\n",
    "            print(\"Le Pokémon sera favori\")\n",
    "            # Ajouter les pokémons recommandés à recommandations \n",
    "            for pokemon_id, pokemon_data in data.items():\n",
    "                if (pokemon_data[\"couleur dominante\"] == couleur \n",
    "                        and pokemon_data[\"tags\"][\"Generation\"] == generation \n",
    "                        and ((pokemon_data[\"tags\"][\"Type 1\"] == type1 and pokemon_data[\"tags\"][\"Type 2\"] == type2) \n",
    "                        or (pokemon_data[\"tags\"][\"Type 1\"] == type2 and pokemon_data[\"tags\"][\"Type 2\"] == type1)) \n",
    "                        and not(pokemon_data[\"tags\"][\"Legendary\"]^legendary)):\n",
    "                    recommandations.append(pokemon_data)\n",
    "\n",
    "            # Afficher les pokémons recommandés\n",
    "            print(\"Voici les pokémons recommandés:\")\n",
    "            for id in recommandations:\n",
    "                print(recommandations[id][\"tags\"][\"Name\"])\n",
    "\n",
    "            satisfait = True\n",
    "        else:\n",
    "            print(\"Le Pokémon ne sera pas favori\")\n",
    "            satisfait = False\n",
    "\n",
    "    # Afficher les pokémons recommandés\n",
    "    print(\"Voici les pokémons recommandés:\")\n",
    "    for id in recommandations:\n",
    "        print(recommandations[id][\"tags\"][\"Name\"])\n",
    "        img = Image.open(\"./pokemon_jpg/\" + id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_pokemon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test unitaire\n",
    "import unittest\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class TestRecommendPokemon(unittest.TestCase):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e709a68c8bffd8ca64ede960b472aee5cbbab0a7eefe704a97155fcd513aef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
